[{"categories":null,"contents":"These are some of the areas where I have dedicated my efforts when it comes to security:\n All vital and IPR associated systems are protected via MFA Grandular IAM permissions configured for AWS services, users and roles I always adhere to the Least Privilege Principle Web Application Firewall (OWASP) Web application XSS, SQL Injection, encoding, securing cookies, role \u0026amp; claims based authz I always encrypt data at rest and also when in transit I always encrypt systems credentials (including 3rd party's) and always include extra layer of encryption All passwords are hashed To isolated blast radius of environments by separating dev/test \u0026amp; prod into different accounts. I restricted account user access in production When I need to share a password, I do it via keybase and I never accompanied it with username or other identifiable key. I use a password manager \u0026amp; where appropriate share credentials via this route (avoids colleague writing down and exposing during demo) I never show passwords via a communication platform (slack, skype, \u0026hellip;) If I use wifi, I use vyprvpn I educate my colleagues on the above actions \u0026amp; preventitive measures  ","permalink":"http://www.garrardkitchen.com/projects/all/security/","tags":["security","tls","at rest","in transit","waf","hashed","password manager","wifi","owasp","iam","mfa"],"title":"Security"},{"categories":null,"contents":" Assessed whether or not to update existing non-maintained Jekyll site to promote myself Researched other options (e.g. Hugo) Experimented with Hugo and different templaes (focus on resume style) Expedited my learning curve Completed resume rewrite in 3 days Configured GitHub pages and custom domain Configured Google Analytics  ","permalink":"http://www.garrardkitchen.com/projects/all/resume/","tags":["hugo","template configuration","google analytics","short deadline"],"title":"Resume"},{"categories":null,"contents":" Used mainly static content and ported new copy/layout to SAM Serverless Application C# .NET Core Razor Pages solution. Originally WP was in place, so removing MySQL reduced the cost. I created a CI/CD Pipeline using BitBucket. It is now hosted via API Gateway \u0026amp; Lambda and not via an Azure Website.  ","permalink":"http://www.garrardkitchen.com/projects/all/corporate-website/","tags":["serverless","api gateway","serverless application model","ci/cd"],"title":"Corporate Website"},{"categories":null,"contents":" I have implemented several solutions around this space using serverless technology and purpose built services [Glue, Athena, Data Pipeline and DMS]. Tools that I have used to help me develop a solution are Jupyter Labs and Pandas \u0026amp; NumPy.  ","permalink":"http://www.garrardkitchen.com/projects/all/etl/","tags":["etl","glue","s3","pandas","python","numpy","athena","data pipelines"],"title":"ETL"},{"categories":null,"contents":"  I have had the honour and experience of hiring several talented individuals.\nI have tried different approaches to assessing a team member. My secret is that I look for crucial character traits. Having tried different approaches to evaluating a prospects suitability, I have come to the conclusion that in most cases lengthy tests or situations that induce stress, are both unnecessary and unfair. I am of the opinion that if you are faced with that kind of situation then something/someone up stream has messed up and it’s more important to me to know how a person (a) handles that situation and (2) how they identify learning points.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/recruitment/","tags":["recruitment"],"title":"Recruitment"},{"categories":null,"contents":"  Coach has a long standing feature called IntelliSearch.\nIt is a feature that, at a specific time, for several set periods [daily, weekly, monthly, custom], it will submit queries to a Data Connector (conduit between our system and an external system) to match up randomly selected recordings for later evaluation. The random algorithm implemented is there to ensure fairness of recording selection so apples and apples are compared and not apples to oranges.\nThis has evolved over time to manage demand and reduced traffic bandwidth.\n  With CXcoach (cloud SaaS product), it was the prime candidate to be migrated to serverless. I architected the system and with the team we implemented the solution. The initial feature is designed for real-time processing so whenever there is a published event from the integrated system, the product obtains the latest information and at the appropriate time, it is loaded into ElasticSearch. The entire process is orchestrated using Step Functions, Lambdas, API Gateway, ElasticSearch \u0026amp; S3.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/intellisearch/","tags":["step functions","lambdas","api gateway","elasticsearch","s3","windows services","akka.net","nats","performance improvements"],"title":"IntelliSearch"},{"categories":null,"contents":" Serverless is ideal for so many use cases. Automation is a prime example of where serverless technology can fit so well. I have used Lambdas and supporting services to help automating both mundane and configuration tasks.\nSome examples of these are; setting log retention durations on log group creations, log subscription and adding API Gateway API keys to SSM Parameter Store.  ","permalink":"http://www.garrardkitchen.com/projects/all/automation/","tags":["serverless","lambda","cloudwatch","aws ecs fargate"],"title":"Automation"},{"categories":null,"contents":"  Migrate Coach product away from a monolyth to being 100% serverless. Main web platform hosted via ECS Fargate and all features, RESTul APIs be Lambda backed API Gateways and Step Functions. Data repositories used are dynamoDB, ElasticSearch, Amazon Aurora (mySQL), Redis and S3.\n  Coach migrated from MSSQL to MySQL as costs were too high for Always-On capabilities. I was accepted onto Aurora MySQL Preview programme and contributed to this phase. The product had a key dependency that required a specific version of MySQL. As a result we are now using Amazon Aurora and plan to migrate over to Aurora Serverless when version 5.7 is supported.\n  To offer CXcoach to a greater audience, a web plug-in dependency had to be removed. This plug-in being Silverlight. Web service technology required for Silverlight had to be replaced with a more traditional RESTful API technology. This, with an reimagined UX, helped deliver a new experience.\n  I orchestrated the entire migration of a solution away from all Microsoft product dependencies. This drastic step was born out of being fed up with (a) overpriced software (e.g. MSSQL, especially in a highly available configuration) and customers having to spend huge amounts to use our products, (b) misdirection of technology (e.g. Silverlight, need I say more?!) and (3) general lack of emphathy with IT professionals who prescribe to doctrine of the MS stack. I decided some time ago that if I get the opportunity too, I will in all respects, pivot away from MS. The areas where it did not make sense to jump ship from is languages and development tools. I am a huge fan of C# \u0026amp; F# and I believe we [subjective I know] are all better of with having VS \u0026amp; VSCode in our lives.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/migrations/","tags":["serverless","ecs fargate","lambda","api gateway","step functions","silverlight","mssql","restful api","asp.net mvc","asp.net core","aws aurora","ux","aws dms","too expensive","microsoft"],"title":"Migrations"},{"categories":null,"contents":"  I have researched several big data solutions but found them either too costly, lead time too lengthy or too complicated.\nCoach needed something to present actionable data. Not knowing whether the idea matched reality, I proposed a phase 1 release that would lead to feature enhancements that would naturally lead to something that would help more. I architected an ETL and data analytics process that would generate separate JSON formatted data, persisted to S3 for a specific user after being processed by a Pandas dataframe. Implemented using CloudWatch trigger, S3, Redis \u0026amp; lambdas and Amazon Aurora (MySQL).\n  ","permalink":"http://www.garrardkitchen.com/projects/all/dashboard/","tags":["etl","lambda","pandas","jupyter labs","python","c#","asp.net core","s3","sqs","redis","cloudwatch trigger","amazon aurora"],"title":"Dashboard"},{"categories":null,"contents":" For me, CX (Customer Experience) is paramount. Even if there is an outage, the CX doesn’t have to suffer. In my current project, there are key AWS services, as well as the CXcoach product, that are required to be running for the application to function. I have architected both full systems and application features in such a way for them not to fail because of transient outages or worse and furthermore to provide instant feedback to users. This also includes not losing anything that has yet to be persisted. I architected and implemented a serverless solution to check at regular intervals the state of key application services and other endpoints (e.g. app health check). If an outage is experienced the ALB is switched to a Lambda backed LB where the user is provided with information up until the time the service(s) is back online. This is all hooked into Slack and OpsGenie.  ","permalink":"http://www.garrardkitchen.com/projects/all/health/","tags":["cx","cloudwatch trigger","lambda","alb","python","lambda load balancer target","elasticsearch","http","amazon aurora","s3","redis"],"title":"Health of the SaaS product"},{"categories":null,"contents":" I have designed and implemented several billing mechanisms. Including designs/models orientated around licensing, another using AWS MMS (metering service) for the AWS QuickStart and more recently, one to capture product usage based on a specific frequency. The later architectured and delivered once again via serverless technology. Created web apps [some serverless] to provide visibility into usage Create application features to help mitigate restrictions on internet access Subscription models designed and developed for: per agent, per user, per period, per feature usage, per role  ","permalink":"http://www.garrardkitchen.com/projects/all/billing/","tags":["windows services","aws metering service","s3","dynamodb","finance portal","lambda","asp.net framework 4.6",".net core"],"title":"Billing"},{"categories":null,"contents":"  An important part of a SaaS application, especially one that requires integrations to provide any measure of benefit, is onboarding.\nFor me, the advantages are simple.\n  Generally, CX needs to be smooth [quick and useful]. It is also important to know at what stage a trialer is at. This can then be aided by a digital campaign with support.\n  With most onboarding models requiring a starting point outside of our product, e.g. vendor, it is even more important that the trialer does not experience any pain during this process; even if they do experience a problem [e.g. outage].\nI set about designing an onboarding process and experience and presented it to the POs (product owners), emphasising the importance of how the initial experience of a new product to a user will determine their engagement, desirability for the product and their colleagues. The onboarding process is designed to be 100% serverless. I also designed enhancements to the product to provide user advice, based on role, as to where they were, during the stages of the product and advice on what they ought to do next, supported by an email campaign.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/onboarding/","tags":["ses","pinpoint","campaign","ux","ux"],"title":"Onboarding"},{"categories":null,"contents":"  Formally a front-end developer, I feel confortable and confident carrying out work in this space\n  The CXcoach cloud based product needed a ‘face lift’.\nI investigated the most frugal way to provide the product with a new look and feel, one that is aligned with industry expectations. I settled on an open source theme and provided my UX lead developer a branch of code that used this new theme for him to complete and fix any style and placement issues. For specific areas of the product (features), I provided the same person with a mock up (HTML and style) for him to incorporate into the React front-end.\n  I was a front-end developer previously and felt confident that what I had proposed could be implemented within an accepted deadline\n  After: Before: ","permalink":"http://www.garrardkitchen.com/projects/all/retheme-product/","tags":["bootstrap","javascript","ux","ui","mock up","asp.net core","c#"],"title":"Retheme Product"},{"categories":null,"contents":"  I created a process for others, external to the company, to perform integrations. The concept is simple and still used today.\nIn summary, a conduit known as a Data Connector was developed to (a) be used by Coach via a RESTful API and (b) integrate with Vendor or other system(s) APIs or data sources. Metadata for users \u0026amp; recordings could then be exchanged via the Data Connector. Additional [configurable] data was also passed along this pipeline. Multiple authentication protocols and approaches were used to permit the flow of data. Data and required credentials were and are always encrypted. The SDK is supported by a sample data connector, API documentation and a test bed via the Azure Platform.\n  Create a developer website to contain all product materials for Coach product and contributed to materials produced\n  Developed integration tool to help integration engineers test their Data Connector\n  Links Sample Data Connector: https://github.com/qualtrak/recorder-qa\nIntegration Tool: http://dev.qualtrak.com/tool\n","permalink":"http://www.garrardkitchen.com/projects/all/sdk/","tags":["integration tool","developer site","sdk","documentation","restful api","dependency injection","data connector"],"title":"SDK"},{"categories":null,"contents":" It is important to me to ensure throughout all my teams we all document, share and have joint responsibility in maintaining guidelines and best practices. It begins when research starts and continues through the projects lifecycle. Documents are maintained in order to provide guidance to both junior and senior team members. Always stress the importance of consistency and standards.  ","permalink":"http://www.garrardkitchen.com/projects/all/best-practice/","tags":["best practice","joint responsibility"],"title":"Best Practice"},{"categories":null,"contents":"  The 3 pillars of observability (logging, metrics and tracing) presents new challenges for serverless solutions.\nHaving experienced this first hand, I set about looking for a solution that will enable rapid root cause analysis to ultimately greatly reduce or even better, mitigate [think runbooks] known undesirable scenarios.\n  I believe in positive Customer Experience (CX) and apply this philosophy across everything I do.\nI spent time with Datadog [a few months of meetings] discussing ways to help this serverless space to improve observability. I was accepted onto a few of their beta programmes. Coach’s cloud solution is 100% serverless which presents a unique challenge for tracing and logging. I liked what Datadog additionally had to offer. The company now uses their new APM for .NET Core and their Cloud Functions feature. Since then, I have, with the rest of the team, adopted a streamlined approach which is more workflow orientated. Using Epsagon for the lambda based solutions, Sentry.io for client-side and server-side error management. All observability solutions were researched and implemented by me. The company’s morning DSU, when necessary, use both Epsagon and Sentry.io to review priorities and where necessary, assign somebody to deal with an issue.\n  All work - greenfield, enhancements and defects - are managed through Jira. All project work and documentation is captured in Confluence. Confluence, like Jira, is very much an important productivity tool embedded into our working protocols.\n  Further applications I use in this area are: Datadog, Epsagon, Sentry, CloudWatch, OpsGenie.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/observability/","tags":["datadog","epsagon","sentry.io","opsgenie","cloudwatch","CX","serverless"],"title":"Observability"},{"categories":null,"contents":"  Created an SaaS version of our product on the AWS platform using serverless tech as an enabler to lowering the running costs and as such, make it cheaper for the vendor/partner to offer our product to their end-clients\n  I have been responsible for driving the integrations with major vendors\n  Made presentations to prospective partners [CEO and below] nd helped shape requirements of the integration\n  Platform required a new set of UX principles\nSome but not all included here: include client backoff retry pattern, ajax abort (propagate cancellation token or asynchronous requests), serverside messaging to loosely coupled components, using browser local storage to store data that is yet to be persisted, warm user of unsaved work before moving away from that page\n  ","permalink":"http://www.garrardkitchen.com/projects/all/cxcoach/","tags":["ecs fargate","lambda","step functions","api gateway","integrations","saas","ux"],"title":"CXcoach"},{"categories":null,"contents":" Created a web based product of our successful client/server product. Chose appropriate tech to fit the timeline. Worked with a Pakistani development outsourcing company and Recorder vendor that the product was integrating with at the time. Later developing the application to integrate with the Redbox Recorder, leading Redbox to purchase the code after an agreed sign off.  ","permalink":"http://www.garrardkitchen.com/projects/all/coach/","tags":["asp.net mvc","silverlight","c#","jquery","angular","html","css","bootstrap","javascript","ajax","restful api","ria services","kendoui","telerik silverlight"],"title":"Coach"},{"categories":null,"contents":" For a particular integration, the necessary data for the integration was not discoverable. I architected a bespoke cloud based solution that used several Azure services. The web app was written with node.js, Azure mobile services, table storage, service bus and Azure Web App (plus staging).  ","permalink":"http://www.garrardkitchen.com/projects/all/qtag/","tags":["node.js","azure mobile services","table storage","service bus","azure web services"],"title":"QTag"},{"categories":null,"contents":" The product, Coach, was in the first wave of SMEs to integrate with the Amazon Connect product. As part of a team, I met with and coordinated with both the AC team and the Marketplace Team. I developed the QuickStart that is hosted via AWS and produced an AMI that is being marketed in the AWS Marketplace. The Coach product was developed to work with Active Directory and I assisted in creating materials to help with all facets of the QuickStart. It was during this process that I was awarded my first AWS Certification - Developer Associate. I worked closely with several members of the AC and Marketplace teams. The QuickStart is a nested stack of CloudFormation [Configuration as Code] templates, incorporating both the AWS VPC and the Data Streaming QuickStarts.  Links QuickStart: https://aws.amazon.com/quickstart/connect/qualtrak-evaluate/\nGitHub: https://github.com/aws-quickstart/connect-integration-qualtrak-evaluate\nMarketplace: https://aws.amazon.com/marketplace/seller-profile?id=d12e19b4-04d3-4e1d-b8a7-0298f3862f33\nAWS What’s New: https://aws.amazon.com/about-aws/whats-new/2018/03/three-new-amazon-connect-integrations-offer-contact-center-solutions-from-qualtrak-solutions-voicebase-and-perficient/\nAWS Solution Space: https://aws.amazon.com/solutionspace/contact-center/qualtrak-evaluate-amazon-connect/\n","permalink":"http://www.garrardkitchen.com/projects/all/aws-quickstart/","tags":["aws quick start","aws marketplace","cloudformation","aws ami","configuration as code","Amazon Connect"],"title":"AWS Quick Start for Amazon Connect"},{"categories":null,"contents":"  Investigated ways to reduce time to market, end-client license costs, hosting costs and development costs.\nThis research and later development efforts contributed to formulating and maintaining a best practice set of guidelines/principals covering all frameworks and languages employed to deliver a solution [product, automation and integrations].\n  Used both Serverless Framework and SAM. I use several IDEs, dependant on task/requirement, mainly VSCode, Rider, PyCharm and VS.\n  These are samples of use cases where I have employed Serverless technologies: Websites (ASP.NET Core Razor), major features (utilising full AWS stack), automation (set log retention via CLoudWatch Events, set Log subscription, Set API Gateway Key and store in SSM).\n  Contribute to Serverless Slack when I can.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/serverless/","tags":["serverless","lambda","api gateway","step functions","serverless framework","serverless application model","reduce cost","reduce time to market"],"title":"Serverless"},{"categories":null,"contents":"  The Coach product started as an on-prem application, installed behind the firewall on a single server.\nA partner made a request for the product [Coach] to be made highly available. Initially the product was hosted via an active/passive configuration. This was superseded by a load balanced active/active configuration through the application of Akka.NET throughout all components. This enabled an application cluster that could load balance the demands across all components. Based on specific feedback, I later re-architected the messaging driven solution and replaced Akka.NET with NATS.\n  I have previous experience of NATS with projects/research I have accomplished with Docker [Windows] and felt confident that this work could be completed in a limited timeframe. This previous research included both the use of C# and F#.\n  The HA solution for Coach is permanently on in a HA configuration on AWS (RDS, Redis, EC2 [windows], ALB, ACM, Route53).\n  Development work is conducted via AWS Workspaces [Windows] as and when required.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/ha/","tags":["nats.io","high availability","aws","single-server","multi-server","CI/CD"],"title":"Streamline deployment of HA app"},{"categories":null,"contents":" I created a culture that removes the responsibility of one person/team/department to manage infrastructure, and to deliver that freedom to the developer/team in order to minimize latency between feature conception and release and to react rapidly to any production issues  ","permalink":"http://www.garrardkitchen.com/projects/all/devops/","tags":["devops","freedom","empowerment"],"title":"DevOps"},{"categories":null,"contents":"  CI has been a long term development responsibility.\nFormally, for our client/server product, I would install and host TeamCity on office servers. Currently for a specific version of Coach, it is now hosted on an Azure instance. The deployment package is delivered to S3 and at the appropriate time (major/minor/patch release) is added to a Confluence site for our customers to download.\nAll projects are now driven by CI/CD principals. This is for both on-premise and cloud solutions. Depending on the project and requirements, different tools have been used. R\u0026amp;D into use and application of these tools was my responsibility including introducing the original idea to the business and implementing across all projects. Tools used include Azure DevOps, BitBucket Pipelines, AWS CodePipeline, AWS CodeBuild, AWS CodeDeploy, Octopus Deploy, Jenkins and TeamCity. All code is held in git repositories. These repositories include BitBucket (default), GitHub and CodeCommit. Each new feature, enhancement or defect is linked to a Jira issue.\n  I have architected several CI/CD solutions using the above listed technologies.\nKnowing that as a company we needed to evolve and advance, I had to start automating a lot more of our activities, I investigated what could be done. I presented my research and delivered my recommendations to the company and embarked on this as being an important tenet to how we automate this space. My original architecture started with Jenkins and Docker. I then promoted and embedded Azure DevOps and the company have used this for several years. More recently, AWS has been used for related services but now mainly for serverless solutions BitBucket Pipelines is used.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/cicd/","tags":["continuous integration","continuous deployment","continuous delivery","bitbucket pipelines","codebuild","codepipeLine","codedeploy","codecommit","github","azure devops"],"title":"Continuous Integration, Continuous Deployment \u0026 Continuous Delivery"},{"categories":null,"contents":"  Liaised with a vendor to create conduit [data connector] between our products and theirs. Worked with many vendors including Twilio (including Flex), Zendesk, Telax, DataVoice, Novo, Comways, Altitude, Dubber, Frequentis, Spitch, Amazon Connect, Redbox.\n  Worked with teams to scope requirements, define every stage up to sign off and team responsibilities. Each integration would start with a technical presentation delivered by me. This would encompass architecture, stages and responsibilities, establishing test environments and training requirements.\n  ","permalink":"http://www.garrardkitchen.com/projects/all/integrations/","tags":["integration","twilio","zendesk","telax","datavoice","novo","comways","altitude","dubber","frequentis","spitch","amazon connect","redbox","world-wide","different cultures","different roles","project scope","training"],"title":"Integrations"},{"categories":null,"contents":"Most Engineers are fully versed in the foundations of writing quality, efficient, succinct and testable code. As a Principal Engineer, one of my responsibilities is to ensure that this baseline is (1) understood and (2) adhered to by all engineers.\nHere's a list of concepts that for me, constitute good engineering principles:\nThese are in alphabetical order and not in order of importance\n Clean and readable code Code reviews Coding standards Composition over inheritance Defensive coding Do no more DRY KISS Occam's razor Premature optimization Refactor Separation of Concerns SOLID Testing YAGNI  Clean and readable code Clean and readable code is always better than clever code (ask any engineer who has to extend or maintain a clever piece of code!)\nI've seen a lot of code recently that should never have got to the shape it has. Complicated code requires time to understand, then time to add functionality. Complicated code also happens to more difficult to recall so each time you need to go near it, you have to relearn it and added to this, any changes made to improve it, most likely have not been applied in full so they'll be a right old mixture of good, bad and the ugly thrown into the mix.\nA good measure of how bad a codebases is, and I'm going to plagiarise somebody else's analogy here, is by stepping through an interactive debug session. If you get momentarily distracted by a fly, then immediately return to the debugging and you do not know where the feck you are in the execution of the code flow, then it's a bad codebase!\nIt's the responsibility of a Tech Lead or architecture to stop code bases ending up this way.\nCode reviews It should only contain helpful and constructive comments and/or implementation questions. This process is not there to caress egos (that's for your mother to do!!). One useful by-product of code reviews is conveying of your team's exacting coding standard and attention to deal, to new starters. So, the quicker the new starter pushes a commit, the better!\nCoding standards (provide a template of core standards then stand back and let the team thrash out the rest - wear protection!)\nAlthough important, it's not the end of the world if some of the granular details differ between teams. The important thing here, in my opinion, is that each team know where to find their cheese. Most engineers in a team have a common set of standards they adhere too. The big things like solution structure, naming conventions, testing (AAA, GWT), pluralization, documentation structure (including README) all need to be consistent.\nComposition over inheritance (avoid class tree exploitation! - think Strategy pattern - GoF)\nThe above-bracketed statement says it all! Inheritance tends to back you into a corner especially when you consider the OCP.\nDefensive coding (guard against invalid class method parameters and accidental null assignment to class properties instead of an equality condition!)\nThis is one example of defensive coding:\nclass User(string firstnaame, string lastname, int age) {\rif (null == firstname) {\rthrow new NullReferenceException(\u0026#34;Firstname cannot be null\u0026#34;)\r}\r...\rThe above demonstrates an example of defensive coding. The first is that we need to test for valid constructor parameter values when instantiating a class.\nThe second, if to avoid mistakes that might not be picked up by your compiler. For instance, a common mistake doing this:\nif (firstname = null)\rA .NET Compiler is more than happy allowing this above syntax, as, after all, it's an assignment operator and not a equality operator as in above in the class constructor. By switching these around, you're making a positive pattern changing and should avoid making this silly mistake again.\nDo no more (and do no less - thank you eXtreme Programming!).\nIf you code outside the scope, you're in danger of creating code that isn't used or needed. The worse thing about this is that others will have to maintain this code. How can this be? Well, it's common - think HTTP chaining - for code not to be culled especially if there is a disconnect between these dependencies and there's no IDE/compiler to shout at you.\nDRY (don't repeat yourself)\nCode analysis tools help here, but you're not always going to have access to these tools.\nOne way to help identify code that does the same thing is by refactoring. If you keep your code method frame small (~20 lines), and you have a good naming standard for methods (e.g. noun+verb with accurate alighment to business capability - think DDD), have unit tests with a high code coverage percentage, then this should be all you need to help you avoid writing duplicate code.\nKISS (keep it simple, silly)\nThis to a certain extent, goes hand in hand with avoiding premature optimization. We all like the big picture yes? This doesn't mean we need to do deliver on this it right now! You just need to know the boundaries of this piece, which, if greenfield, then you won't have any metrics to tell you the actual demand. Think Capacity planning; what this piece of work needs to do based on current expectations. For example\nDo we need multiple servers? Yes, I think Why do we need multiple servers? Mmmmm, because I read it somewhere\rDo you have the metrics that support your argument for multiple servers? Wait, what?\rNext!\r A colleague recently shared with me the architecture of their side project. They are using AWS and I have 2 certifications in AWS (Developer and Solutions Architect). I rapidly went into HA/Scaling/DR overdrive and rapidly did a verbal dump on what tech they should use. This was all wrong - they did not know their service demand. This will have added to their cost; unnecessarily. I did, shortly after, re-affirm their decision (may have made 1 or 2 helpful minor suggestions). Yeah, think big but don't deliver big without a customer base; huge waste of time, effort and money.\nOccam's Razor This is a problem-solving principle.\nThe definition of this is: \u0026ldquo;Entities should not be multiplied without necessity\u0026rdquo;. It is sometimes paraphrased by a statement like \u0026ldquo;the simplest solution is most likely the right one.\nOccam's razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions. Good advice\nSuppose there are two competing theories on why something is not working. Normally, the case that requires the least amount of assumptions is correct. So, the more assumptions you have to make means it more likely to be more unlikely.\nPremature optimization Avoid premature optimization and all conversations relating to optimization until you know the facts. This will be futile until you've metrics to better inform you.\nI've hit this numerous times when planning for microservices and bounded contexts, in particular, on green-field projects. What should we include and where? Should we separate claims away from users for instance? Will the demand for Claims be greater than for users? Who knows?! You don't until you have some metrics behind you. You can always merge or break them [microservices] up later.\nAnother area that I believe this encompasses is splitting code up across multiple files and folders. If it's a PoC, a sample piece of code, or something that has a short shelf life, just keep it in one file. When it's the right time - moving out of PoC/other - then you can consider optimizing it. Up until then, it's a huge waste of time and effort.\nArchitecture is a great example of when not to prematurely optimize. Architecture normally infers cost. Generally, the more of something, the greater the cost. This could mean for a startup the difference between survival and their demise. Adopting a guiding principle of being frugal from the outset, is a prudent and wise decision. What this means is that you're always looking for the most cost-effective way of accomplishing your goal. So, if you don't know your demand, it means you opt for a single server instead of having a HA cluster of 3 master nodes and 5 worker nodes! Down from 8 servers to 1 which on a month by month basis during development and beta/early releases could mean the saving of thousands of pounds sterling.\nSadly, I've come across a few startup that have failed just because they ran out of cash early on. It's a real shame for all involved.\nRefactor \u0026hellip;refactor refactor\nDon't save this until the end of a piece of work \u0026hellip; you're bound to miss something and possibly add to your team's tech debt. Plus, if you push your commits to a PR, you'll get your ass handed to you by your peers!\nThings to consider here are DRY and TDD. Both will nudge you towards a proper refactoring effort.\nSeparation of Concerns (think MVC, CQRS, bounded context, etc\u0026hellip;)\nIt's all about doing the right this in the right place! I recently ran, architected and co-developed a project that involved our own hosted solution, a solution hosted on Azure and a solution hosted on the Twilio Cloud (Twilio Serverless Functions). Originally, the requirements did not include the Twilio Cloud and would have required a bucket load more effort if we'd stuck with that brief. Thankfully, I chose to take full advantage of what Twilio has to offer and used a combination of Twilio Flow and Twilio Serverless Functions. By establishing these SoCs it meant:\n a less stressful implementation a light touch to our own hosted solutions a satisfying amount of fun working with Serverless (has been my favourite and advocated approach for several years!) a time saving it revealed a range of options when dealing with specific edge and corner cases which, again, giving us a further time savings.  SOLID These are the SOLID principles:\n Single Responsibility Principle Open Closed Principle Liskov Principle Interface Segregation Principle Dependency Inversion Principle  Single Responsibility Principle A class (no method) should have one and only one reason to change, meaning that a class (or method) should have only one job.\n\u0026ldquo;When a class has more than responsibility, there are also more reasons to change that class\u0026rdquo;\nHere's an example of a class )purposefully awful for illustrative purposes):\nclass User() {\rpublic string Username {get; set;}\rpublic string Fullname {get; set;}\rprivate readonly ILogger _logger;\rprivate IDbContext _db;\rpublic User()\r{\r_logger = new Logger() _db = new UserContext();\r}\rpublic Task\u0026lt;User\u0026gt; GetProfile(string username) {\r...\r_logger.Info($\u0026#34;Found profie for {username}\u0026#34;)\rreturn this;\r} }\rYou could say that the above includes both a model responsibility and a service responsibility. These should be split into two separate .NET types, as in this example:\nclass User() {\rpublic string Username {get; set;}\rpublic string Fullname {get; set;}\rpublic User(string username, string Fullname) {\r...\r} }\rclass UserService() {\rprivate readonly ILogger _logger;\rpublic UserService(ILogger _logger, IDbContext db)\r{\r_logger = _logger _db = db; }\rpublic Task\u0026lt;User\u0026gt; GetProfile(string username) {\r...\r_logger.Info($\u0026#34;Found profie for {username}\u0026#34;)\rreturn user;\r} }\rHere are the benefits of principles:\n Reduces complexity in your code Increases readability, extensibility, and maintenance of your code Reusability and bug breading Easier to test Reduces coupling by removing dependency between methods  Open Closed Principle Objects or entities should be open for extension, but closed for modification. So, what does this mean? Let's break this down to two statements:\n Open for extension Closed for modification  Open for extension: This means that we need to design our classes in such a way that it's new responsibilities or functionalities should be added easily when new requirements come.\nOne technique for implementing new functionality is by creating new derived classes. These are to inherit from base classes. Another approach is to allow the \u0026lsquo;client\u0026rsquo; to access the original class with an abstract interface. I sometimes see this as removing if statements. Not sure everybody would agree with this assessment though.\nSo, in short, if there's a change in requirement or any new requirements, instead of touching the existing functionality, it is better to create new derived classes and leave the original class implementation. Well, that's the advice! I worry about the class explosion and if you're attempting to do this on top of not so perfect code!\nClosed modification: This is very easy to explain\u0026hellip;only make modifications to code if there's a bug.\nThis sample looks at delegating method logic to derived classes.\npublic class Order {\rpublic double GetOrderDiscount(double price, ProductType productType) {\rdouble newPrice = 0;\rif (productType == ProductType.Food) {\rnewPrice = price - 0.1;\r} else if (productType == ProductType.Hardware) {\rnewPrice = price - 0.5;\r}\rreturn newPrice;\r}\r}\rpublic enum ProductType {\rFood,\rHardward\r}\rCan rewrite, still using base implementation (think decorator pattern):\npublic class Order {\rpublic virtual double GetOrderDiscount(double price) {\rreturn price;\r}\r}\rpublic class FoodOrder : Order {\rpublic override double GetOrderDiscount(double price) {\rreturn base.GetOrderDiscount(price) - 0.1;\r}\r}\rpublic class HardwareOrder : Order {\rpublic override double GetOrderDiscount(double price) {\rreturn base.GetOrderDiscount(price) - 0.5;\r}\r}\rLiskov Principle Definition: \u0026ldquo;Let q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.\u0026rdquo; \u0026hellip; clear as mud right?\nAll this is stating is that every subclass/derived class should be substitutable for their base/parent class.\nThe example below demonstrates a violation of the Liskov principle, as by replacing the parent class (SumEvenNumbersOnly-\u0026gt;Calculator), this does compromise the integrity of the derived class as the higher-order class is not replaced by the derived class. Here, both cal and eventsOnly variables will be the same:\n...\rvar nums = new int[] {1, 2, 3, 4, 5, 6, 7};\rCalculator cal = new Calculator(nums);\rCalculator evensOnly = new SumEvenNumbersOnly(nums);\r...\rpublic class Calculator\r{\rprotected readonly int[] _numbers;\rpublic Calculator(int[] numbers)\r{\r_numbers = numbers;\r}\rpublic int Sum() =\u0026gt; _numbers.Sum();\r}\rpublic class SumEvenNumbersOnly : Calculator\r{\rpublic SumEvenNumbersOnly(int[] numbers) : base(numbers)\r{\r}\rpublic new int Sum() =\u0026gt; _numbers.Where(x=\u0026gt;x % 2 == 0).Sum();\r}\rHere we have changed the assumed base class to an abstract class. Now, it can't be instantiated and instead, must be inherited. This ensures the derived classes must implement the method detail. So, even if we replace the type declaration with the higher-order class, we should still get the intended result:\n...\rvar nums = new int[] {1, 2, 3, 4, 5, 6, 7};\rCalculator cal = new SumAllNumbersOnly(nums);\rCalculator evensOnly = new SumEvenNumbersOnly(nums);\r... public abstract class Calculator\r{\rprotected IEnumerable\u0026lt;int\u0026gt; _num;\rprotected Calculator(IEnumerable\u0026lt;int\u0026gt; num)\r{\r_num = num;\r}\rpublic abstract int Sum();\r}\rpublic class SumAllNumbersOnly : Calculator\r{\rpublic SumAllNumbersOnly(IEnumerable\u0026lt;int\u0026gt; num) : base(num)\r{\r}\rpublic override int Sum() =\u0026gt; _num.Sum();\r}\rpublic class SumEvenNumbersOnly : Calculator\r{\rpublic SumEvenNumbersOnly(IEnumerable\u0026lt;int\u0026gt; num) : base(num)\r{\r}\rpublic override int Sum() =\u0026gt; _num.Where(x =\u0026gt; x % 2 == 0).Sum();\r}\rInterface Segregation Principle A client should never be forced to implement an interface that it doesn't use or clients shouldn't be forced to depend on methods they do not use.\nTake the following interface:\npublic interface IAllTheThings {\rTask\u0026lt;IAsyncEnumerable\u0026lt;Claim\u0026gt;\u0026gt; GetClaims(string username);\rTask\u0026lt;IAsyncEnumerable\u0026lt;User\u0026gt;\u0026gt; GetUsers(string team);\rTask\u0026lt;User\u0026gt; AddUsers(User user);\r}\rThere's a clear distinction in responsibilities that are being suggested here by the contract name. Sufficed to say, these should be split across different interfaces:\npublic interface IUser { Task\u0026lt;IAsyncEnumerable\u0026lt;User\u0026gt;\u0026gt; GetUsers(string team);\rTask\u0026lt;User\u0026gt; AddUsers(User user);\r}\rpublic interface IClaim\r{\rTask\u0026lt;IAsyncEnumerable\u0026lt;Claim\u0026gt;\u0026gt; GetClaims(string username); }\rDependency Inversion Principle There are 2 rules here:\n High-level modules should not depend on lower-level modules. Both should depend on abstractions. Abstractions should not depend upon details. Details should depend upon abstractions.  Let's deal with the first rule first. High-level means policy, business logic and the bigger picture. Lower-level means, closer to the bare metal (think I/O, networking, Db, storage, UI, etc\u0026hellip;). Lower-level tend to change more frequently too.\nThese two examples show perfectly the before and after of the move to a \u0026lsquo;depend on abstraction\u0026rsquo;:\npublic class BusinessRule {\rprivate DbContext _context; public BusinessRule() {\r_context = new DbContext();\r}\rpublic Rule GetRule(string ruleName) {\r_context.GetRuleByName(ruleName);\r}\r}\rpublic class DbContext { public DbContext() { }\rpublic Rule GetRuleByName(string name) {\rreturn new Rule(new {Name = \u0026#34;Allow All The Things\u0026#34;, Allow = false})\r}\r}\rAfter changing to an abstraction:\npublic interface IDbContext {\rRule GetRuleByName(string name);\r}\rpublic class BusinessRule {\rprivate IDbContext _context; public BusinessRule(IDbContext context) {\r_context = context;\r}\rpublic Rule GetRule(string ruleName) {\r_context.GetRuleByName(ruleName);\r}\r}\rpublic class DbContext : IDbContext\r{ public DbContext() { }\rpublic Rule GetRuleByName(string name) {\rreturn new Rule(new {Name = \u0026#34;Allow All The Things\u0026#34;, Allow = false})\r}\r}\rWith the above change, the DbContext can be any class as long as it inherits from the IDbContext interface and has a method with a signature of Rule GetRuleByName(string name).\nThe above is demonstrative of the 2nd rule; do not depend on the detail. As you can see, in the example above, we're depending on an interface method contract and the actual implementational detail is being dealt with by the Lower-level class.\nThe above example includes Dependency Injection. Although you can accomplish IoC with DI, they are not the same thing. IoC does not mention anything about the direction of the dependency.\nGeneralization restrictions The presence of interfaces to accomplish the Dependency Inversion Pattern (DIP) has other design implications in an object-oriented program:\n All member variables in a class must be interfaces or abstracts All concrete class packages must connect only through interface or abstract class packages No class should derive from a concrete class No method should override an implemented method All variable instantiation requires the implementation of a creational pattern such as the factory method or the factory pattern, or the use of a dependency-injection framework.   My pattern discovery I'm a huge fan of patterns, especially cloud architectural patterns but sometimes, they add unnecessary complicity so beware!\nWhen I first started learning about patterns - some 18 years ago - I went through a few codebases I was involved with at the time to see if I'd subconsciously been repeatedly using a pattern \u0026hellip; and I had! It was the lazy loading pattern\u0026hellip;which I continue to use regularly today!\nTesting (unit/functional, including concepts like TDD \u0026amp; BDD and frameworks)\nFor testing to be a success, the details are key. These details will come in the form of a specification or from a verbal conversation (always to be confirm in writing later). If you're lucky, these test cases will be included as ACs (Acceptance Criteria) in the Scrum Story Description.\nTaking a test driven development approach to writing code often results in:\n a reduction in verbose code less post-deployment bug fixing succinct (do no more, no less than is required), structure and logic.  Testing is important. Obviously! I often refer to testing as \u0026lsquo;having your back\u0026rsquo;. It ensures you don't break existing functionality when implementing new functionality or dealing with tech debt. It also protects new engineers from breaking things as well as extant engineers who may have touched this repository many times before.\nTests aren't just for new functionality either. If you change extant functionality or class responsibilities you must modify extant tests or create new tests. Ideally, your CI build pipeline should run tests every time time a commit(s) is pushed to a PR or Draft PR. This last step is here, to again, have your back and to safeguard against erroneous code poluting your codebases and getting into production.\nIn the .NET world, there are many testing frameworks available; xUnit, NUnit, MSTest to name a few. There are also many mocking frameworks available; Moq, NSubstitute, JustMock, again, to name a few. Frameworks like these help make the testing process and overall experience less painful and cumbersome and some might even say it makes this part of development, pleasurable!\nMy .NET Core testing and mocking preferences are xUnit \u0026amp; Moq and my javascript (including node.js) testing framework preference is Jest.\nThis code sample shows how both a testing and mocking frameworks compliment each other:\nusing Moq;\rusing Xunit;\rnamespace test1\r{\rpublic interface IUser\r{\rstring GetFullname();\rstring Firstname { get; set; }\rstring Lastname { get; set; }\r}\rpublic class User : IUser\r{\rpublic string Firstname { get; set; }\rpublic string Lastname { get; set; }\rpublic string GetFullname()\r{\rreturn $\u0026#34;{Firstname} {Lastname}\u0026#34;;\r}\r}\rpublic class Notify\r{\rprivate IUser _user;\rpublic Notify(IUser user) =\u0026gt; _user = user;\rpublic string GetMessage() =\u0026gt; $\u0026#34;{_user.GetFullname()} has been notified\u0026#34;;\r}\rpublic class NotifyTests\r{\r[Theory]\r[InlineData(\u0026#34;Garrard\u0026#34;, \u0026#34;Kitchen\u0026#34;, \u0026#34;Garrard Kitchen has been notified\u0026#34;)]\r[InlineData(\u0026#34;Charles\u0026#34;, \u0026#34;Kitchen\u0026#34;, \u0026#34;Charles Kitchen has been notified\u0026#34;)]\rpublic void GivenGetFullnameCalled_WhenFirstAndLastNameExits_ThenReturnsAValidFullname(string firstname,\rstring lastname, string expected)\r{\r// arrange\r var user = new Mock\u0026lt;IUser\u0026gt;();\ruser.Setup(x =\u0026gt; x.GetFullname()).Returns($\u0026#34;{firstname} {lastname}\u0026#34;);\rvar sut = new Notify(user.Object);\r// act\r string message = sut.GetMessage();\r// assert\r Assert.Equal(expected, message);\ruser.Verify(x =\u0026gt; x.GetFullname(), Times.Once);\r}\r}\r}\rYAGNI (you ain't going to need it)\nDo no more, and no less than is required. You do not want to have to maintain code that is never used or produce code that others have to maintain unwittingly. It's very difficult to future proof your code if you do not know what's going to happen, let alone without a specification! It's a guess at best so don't waste your time or others. Keeps things concise, succinct and simple.\n Being a Principle Engineer As a Principal Engineer, I consider the above as the foundation for writing quality code. The objective of this list, in conjunction with the message I propagate via this list, during discussions, evidence from my own work and by leading from the front within my role, is one of a reminder to me and my colleagues of best practice and commitment to quality and good practice. As with all foundations, it forms the base from which more advanced concepts or approaches can be learned. An important part of this practice is heuristic - enabling a person to discover or learn something by themselves. So, how do I go about doing this?\nThese are some of the activities I execute to embed good engineering principles:\n 1-2-1 Group conversations Advocate online learning platforms such as Pluralsight or Udemy. For the more keen Engineer, I also recommend certification. YouTube is another favourite of mine. With YouTube, you can tag recordings, therefore building up a catalogue of materials that you can make public Workshops Brown bags Capture How To Do's in wikis or similar Coding advice/tips (e.g. when to use Task instead of an Async method) Take the time to explain questions about implementation reasons in DVCS Pull Requests Share blog posts \u0026amp; other materials across multiple channels Compile a learning profile for an individual  The coding advice/tips above are interesting ones. As professionals, we always want to improve our ability to code etc and in doing so we want our colleagues to benefit from our knowledge too. I recently became reacquainted with coding katas. As a black belt in Ju-Jitsu I am well versed in what a kata is. Katas can also be used to remind, stretch and improve our core coding capability. The last time I used a kata in programming was 10+ years ago. This was when I was first introduced to TDD. A favourite development book of mine is \u0026lsquo;The Art of Unit Testing\u0026rsquo; by Roy Osherove. It was the first edition. For many years I had it as a click-thru purchase option on a previous site of mine. I've not really participated in many katas since. I have written a few though and now having been reintroduced to them and reminded of their potential, as a Principal Engineer I can see it as an invaluable tool for both providing the framework to assess an Engineer's current capability and as an aid to building an Engineer's skill when used with pair programming.\nPair programming is a wonderful technique for propagating skills. Quite often, I find I only participate in pair programming is one of two use cases. (1) if the subject I'm investigating is new (important to have shared knowledge) and (2) when I'm helping an Engineer to overcome an esoteric issue. You know what they say? \u0026hellip;a problem shared is a problem halved! However, now, I'll be including Pair Programming as part of my techniques to stretch the Engineer's muscle memory (including mine!).\nDiscussion point \u0026hellip;\nI'm sure I'm not alone here when I say, having the time available for 2 Engineers to code together for skills transfer etc is a challenging one. An agile sprint doesn't really facilitate this. This is something that I often refer to as having the \u0026lsquo;space to learn\u0026rsquo;. The pressures of a sprint often, sadly, works against this. This is doubly as difficult, if your sprint is made up of technical debt, BAU, Ad hoc etc\u0026hellip; Timeboxing \u0026lsquo;effort\u0026rsquo; into percentages doesn't always present an obvious education path for your Engineers either. Having a day (developer day or similar) dedicated to learning also never really quite works out the way it's meant too, plus, \u0026lsquo;a day\u0026rsquo;?! In my experience this, and trying to cram genius into a time box also never quite works either. After all, you can't schedule genius, in the same way you can't guarantee that the best Engineers are in your locality, or that the best time for Engineers to work is between 9-5.\nWhat is the answer? A mixture of all the above, at hock and at scheduled times, to ensure quality and advancement of skills.\nWhen I do speak out regarding the above, I inevitably also lead this conversation into Engineering not having the kit [hardware \u0026amp; software] they need. Engineers require the software and hardware they deem as necessary to be effective in their role. I once gave an analogy of, not giving Engineers the right kit is like giving a roller brush and a Pogo stick to Michelangelo to paint the Sistine Chapel ceiling. He'll manage it \u0026hellip; eventually, but the attention to detail and accuracy will be woefully inadequate.\nWritten mainly for me, I do hope you've found something to take away with you to help you along with your engineering journey.\nReferences:   The pragmatic programmer\n  YAGNI\n  XP\n  Dependency inversion principle\n  OOP design patterns\n  Inversion of Control Containers and the Dependency Injection pattern\n  ","permalink":"http://www.garrardkitchen.com/blog/principles/","tags":["Hugo","blogging","good engineering","principles"],"title":"Good Engineering - Principles"}]